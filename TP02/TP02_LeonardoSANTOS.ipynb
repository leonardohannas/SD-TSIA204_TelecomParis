{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2017d354",
   "metadata": {},
   "source": [
    "# SD-TSIA204 - Statistics: Linear Models\n",
    "## TP 2: Linear Regression\n",
    "### Leonardo HANNAS DE CARVALHO SANTOS\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf73297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change here using YOUR own first and last names\n",
    "fn1 = \"Leonardo\"\n",
    "ln1 = \"Hannas\"\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(),\n",
    "                        [\"SD-TSIA204_lab2\", ln1, fn1])) + \".ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a74e83",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb90781",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "**For the ﬁrst question, we load a standard dataset from `sklearn.datasets` named `fetch_california_housing`. This dataset has only $p = 8$ variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "816cc6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  Target  \n",
       "0    -122.23   4.526  \n",
       "1    -122.22   3.585  \n",
       "2    -122.24   3.521  \n",
       "3    -122.25   3.413  \n",
       "4    -122.25   3.422  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "df['Target'] = housing.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584d7af2",
   "metadata": {},
   "source": [
    "### Question 1.a\n",
    "**Estimate the coeﬃcients with the expression of the normal equaitons seen in class. Code two functions to compute the MSE and the R2 coeﬃcient and compare them with the version of `sklearn` for the train and the test sets.**\n",
    "\n",
    "To derive the nornal equation, we start from the minimization of the cost function for linear regression:\n",
    "$$\n",
    "\\hat \\theta = \\arg\\min_{\\theta \\in \\mathbb{R}^d} J(\\theta)\n",
    "            = \\arg\\min_{\\theta \\in \\mathbb{R}^d} \\frac{1}{2} \\left\\| \\hat y - y \\right\\|^2\n",
    "            = \\arg\\min_{\\theta \\in \\mathbb{R}^d} \\frac{1}{2} \\left\\| X \\theta - y \\right\\|^2\n",
    "$$\n",
    "\n",
    "By taking the gradient of the cost function and setting it to zero, we obtain:\n",
    "$$\n",
    "\\begin{align*}\n",
    "J(\\theta + \\delta \\theta) &= \\frac{1}{2} \\left\\| X \\left(\\theta + \\delta \\theta\\right) - y \\right\\|^2\n",
    "                          = \\frac{1}{2} \\left\\| \\left(X \\theta - y\\right) + X \\delta \\theta \\right\\|^2\\\\\n",
    "                          &= \\frac{1}{2} \\left\\| X \\theta - y \\right\\|^2 + \\left < X \\theta - y, X \\delta \\theta \\right> + \\frac{1}{2} \\left\\| X \\delta \\theta \\right\\|^2\\\\\n",
    "                          &= J(\\theta) + \\left < X^T \\left(X \\theta - y\\right), \\delta \\theta \\right> + \\frac{1}{2} \\left\\| X \\delta \\theta \\right\\|^2\\\\\n",
    "                          &= J(\\theta) + \\left < X^T \\left(X \\theta - y\\right), \\delta \\theta \\right> + o(\\left\\| \\delta \\theta \\right\\|)\\\\ \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore, the gradient of the cost function is given by:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = X^T \\left(X \\theta - y\\right)\n",
    "$$\n",
    "\n",
    "Setting the gradient to zero, we obtain the normal equation to find the minimum, the OLS estimator $\\hat\\theta$:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = X^T \\left(X \\hat\\theta - y\\right) = 0 \\iff \n",
    "X^T X \\hat\\theta - X^T y = 0 \\iff \n",
    "X^T X \\hat\\theta = X^T y \\iff \n",
    "\\boxed{\\hat\\theta = (X^T X)^{-1} X^T y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6d14a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated coefficients (θ̂):\n",
      "- θ̂_0: [-36.60959377910763]\n",
      "- θ̂_1: [0.4390910418766558]\n",
      "- θ̂_2: [0.00959864664827047]\n",
      "- θ̂_3: [-0.10331117273683757]\n",
      "- θ̂_4: [0.6167301519262859]\n",
      "- θ̂_5: [-7.632751973827415e-06]\n",
      "- θ̂_6: [-0.004488382564146345]\n",
      "- θ̂_7: [-0.41735328391314763]\n",
      "- θ̂_8: [-0.4306144620930724]\n"
     ]
    }
   ],
   "source": [
    "# Separating dependent and independent variables\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Adding a column of ones in the feature matrix\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Computing the OLS coefficients\n",
    "theta_hat = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train\n",
    "print(\"Estimated coefficients (θ̂):\")\n",
    "for i, theta_i in enumerate(theta_hat):\n",
    "    print(f\"- θ̂_{i}: [{theta_i}]\")\n",
    "\n",
    "# Prediction of the dependent variable\n",
    "y_hat = X_test @ theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b94f2",
   "metadata": {},
   "source": [
    "To measure the performance of our linear regression model, we can compute the Mean Squared Error (MSE) and the R-squared (R²) coefficient.\n",
    "* The MSE is defined as:\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "* The R² coefficient is defined as:\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2},\n",
    "$$ \n",
    "where $\\bar{y}$ is the mean of the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a837dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error Comparison:\n",
      "- Coded Mean Squared Error Function = 0.5404128061707798\n",
      "- Mean Squared Error from Sklearn = 0.5404128061707798\n"
     ]
    }
   ],
   "source": [
    "def MSE(y, y_hat):\n",
    "    return np.mean((y-y_hat)**2)\n",
    "\n",
    "print(\"Mean Squared Error Comparison:\")\n",
    "print(f\"- Coded Mean Squared Error Function = {MSE(y_test, y_hat)}\")\n",
    "print(f\"- Mean Squared Error from Sklearn = {mean_squared_error(y_test, y_hat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ffdef53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score Comparison:\n",
      "- Coded R2 Score Function = 0.5911695436411457\n",
      "- R2 Score from Sklearn = 0.5911695436411457\n"
     ]
    }
   ],
   "source": [
    "def R2(y, y_hat):\n",
    "    return 1 - np.sum((y-y_hat)**2)/np.sum((y-np.mean(y))**2)\n",
    "\n",
    "R2(y_test, y_hat)\n",
    "print(\"R2 Score Comparison:\")\n",
    "print(f\"- Coded R2 Score Function = {R2(y_test, y_hat)}\")\n",
    "print(f\"- R2 Score from Sklearn = {r2_score(y_test, y_hat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398f15a1",
   "metadata": {},
   "source": [
    "### Question 1.b\n",
    "**Finally, give the conﬁdence intervals at level $99\\%$ for all the coeﬃcients coding the expression for the CI seen in session 3.**\n",
    "\n",
    "In order to do this, we consider a gaussian model:\n",
    "$$T_j=\\frac{\\widehat{\\theta}_j-\\theta_j^*}{\\widehat{\\sigma} \\sqrt{\\left(X^{\\top} X\\right)_{j, j}^{-1}}} \\sim \\mathcal{T}_{n-p-1}$$\n",
    "\n",
    "\n",
    "$t_{1-\\alpha / 2}$ a quantile of order $1-\\alpha / 2$ of the distribution $\\mathcal{T}_{n-p-1}$\n",
    "\n",
    "\n",
    "$$\n",
    "\\left[\\widehat{\\theta}_j-t_{1-\\alpha / 2} \\widehat{\\sigma} \\sqrt{\\left(X^{\\top} X\\right)_{j, j}^{-1}}, \\widehat{\\theta}_j+t_{1-\\alpha / 2} \\widehat{\\sigma} \\sqrt{\\left(X^{\\top} X\\right)_{j, j}^{-1}}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474e6bf",
   "metadata": {},
   "source": [
    "To build this confidence interval, we recall some aspects of the OLS regression model.\n",
    "\n",
    "1. The model is described by the equation $y = X \\theta^* + \\varepsilon$, where $\\theta^*$ is the ground truth parameter vector and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$ is the noise term.\n",
    "\n",
    "2. Dimensions of the matrices and vectors are as follows:\n",
    "* $y \\in \\mathbb{R}^n$ is the observations vector;\n",
    "* $X \\in \\mathbb{R}^{n \\times (p+1)}$ is the design matrix (including the intercept term – first column of ones);\n",
    "* $\\theta^* \\in \\mathbb{R}^{p+1}$ is the true parameter vector to be estimated;\n",
    "* $\\varepsilon \\in \\mathbb{R}^n$ is the noise vector.\n",
    "\n",
    "3. The unbiased OLS estimator $\\hat \\theta \\sim \\mathcal{N}\\left(\\theta^*, \\sigma^2 (X^\\top X)^{-1}\\right)$. Therefore, $\\left(\\hat \\theta - \\theta^*\\right) \\sim\\mathcal{N}\\left(0, \\sigma^2 (X^\\top X)^{-1}\\right)$.\n",
    "\n",
    "4. For full rank $X$, $\\left( n-p-1 \\right) \\hat \\sigma^2 / \\sigma^2 \\sim \\chi^2_{n-p-1}$.\n",
    "\n",
    "5. The estimator for the variance of the noise is given by $\\hat \\sigma^2 = \\frac{1}{n-p-1} \\sum_{i=1}^n \\left(y_i - \\hat y_i\\right)^2$.\n",
    "\n",
    "6. We also remember that the t-distribution can be defined as the ratio of a standard normal variable and the square root of a chi-squared variable divided by its degrees of freedom:\n",
    "$$\n",
    "\\mathcal{T}_k = \\frac{\\mathcal{N}(0, 1)}{\\sqrt{\\chi^2_k / k}}.\n",
    "$$\n",
    "\n",
    "7. With that, we can derive the distribution of the statistic $T_j$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "T_j &= \\frac{\\widehat{\\theta}_j-\\theta_j^*}{\\widehat{\\sigma} \\sqrt{\\left(X^{\\top} X\\right)_{j, j}^{-1}}} \n",
    "= \\frac{\\widehat{\\theta}_j-\\theta_j^*}{\\sigma \\sqrt{\\left(X^{\\top} X\\right)_{j, j}^{-1}}} \\cdot \\frac{\\sigma}{\\widehat{\\sigma}} \\\\\n",
    "&= \\frac{\\widehat{\\theta}_j-\\theta_j^*}{\\sigma \\sqrt{\\left(X^{\\top} X\\right)_{j, j}^{-1}}} \\cdot \\sqrt{\\frac{n-p-1}{\\sum_{i=1}^n \\left(y_i - \\hat y_i\\right)^2 / \\sigma^2}} \\\\\n",
    "&= \\frac{\\widehat{\\theta}_j-\\theta_j^*}{\\sigma \\sqrt{\\left(X^{\\top} X\\right)_{j, j}^{-1}}} \\cdot \\sqrt{\\frac{n-p-1}{\\chi^2_{n-p-1}}} \\\\\n",
    "&= \\frac{\\mathcal{N}(0, 1)}{\\sqrt{\\chi^2_{n-p-1} / (n-p-1)}} \\sim \\mathcal{T}_{n-p-1} \\\\\n",
    "&\\text{Therefore, under the Gaussian assumption, }\\boxed{T_j = \\frac{\\widehat{\\theta}_j-\\theta_j^*}{\\widehat{\\sigma} \\sqrt{\\left(X^{\\top} X\\right)_{j, j}^{-1}}} \\sim \\mathcal{T}_{n-p-1}}\n",
    "\\end{align*}\n",
    "$$  \n",
    "\n",
    "8. Finally, noting $t_{1-\\alpha/2}$ as the quantile of the Student's t-distribution with $n-p-1$ degrees of freedom, we can build a $(1-\\alpha)$-confidence interval for $\\theta_j^*$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "1 - \\alpha &= \\mathbb{P}\\left(-t_{1-\\alpha/2} \\leq T_j \\leq t_{1-\\alpha/2}\\right)  \\\\\n",
    "           &= \\mathbb{P}\\left(-t_{1-\\alpha/2} \\leq \\frac{\\widehat{\\theta}_j-\\theta_j^*}{\\widehat{\\sigma} \\sqrt{\\left(X^{\\top} X\\right)_{j, j}^{-1}}} \\leq t_{1-\\alpha/2}\\right) \\\\\n",
    "           &= \\mathbb{P}\\left(-t_{1-\\alpha/2} \\widehat{\\sigma} \\sqrt{\\left(X^{\\top} X\\right)_{j, j}^{-1}} \\leq \\widehat{\\theta}_j - \\theta_j^* \\leq t_{1-\\alpha/2} \\widehat{\\sigma} \\sqrt{\\left(X^{\\top} X\\right)_{j, j}^{-1}}\\right) \\\\\n",
    "           &= \\mathbb{P}\\left(-\\widehat{\\theta}_j - t_{1-\\alpha/2} \\widehat{\\sigma} \\sqrt{\\left(X^{\\top} X\\right)_{j, j}^{-1}} \\leq -\\theta_j^* \\leq -\\widehat{\\theta}_j + t_{1-\\alpha/2} \\widehat{\\sigma} \\sqrt{\\left(X^{\\top} X\\right)_{j, j}^{-1}}\\right) \\\\\n",
    "           &= \\mathbb{P}\\left(\\widehat{\\theta}_j - t_{1-\\alpha/2} \\widehat{\\sigma} \\sqrt{\\left(X^{\\top} X\\right)_{j, j}^{-1}} \\leq \\theta_j^* \\leq \\widehat{\\theta}_j + t_{1-\\alpha/2} \\widehat{\\sigma} \\sqrt{\\left(X^{\\top} X\\right)_{j, j}^{-1}}\\right) \\\\\n",
    "\\text{Thus, the confidence interval for $\\theta_j^*$ is given by: } &\\boxed{\\left[\\widehat{\\theta}_j - t_{1-\\alpha/2} \\widehat{\\sigma} \\sqrt{\\left(X^{\\top} X\\right)_{j, j}^{-1}}, \\widehat{\\theta}_j + t_{1-\\alpha/2} \\widehat{\\sigma} \\sqrt{\\left(X^{\\top} X\\right)_{j, j}^{-1}}\\right]}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2238c0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.   Degrees of freedom for the t-distribution (n-p-1): 15471\n",
      "\n",
      "2.1. Estimated variance of the error term (σ²): 0.5195291202652413\n",
      "\n",
      "2.2. Estimated standard deviation of the error term (σ): 0.7207836847940173\n",
      "\n",
      "3.   t-value for a 99% confidence interval: 2.5761471316597158\n",
      "\n",
      "4.   Standard error of the coefficients (σ_hat √[X.T @ X]^-1_(j,j)):[7.59446657e-01 4.84426691e-03 5.12919391e-04 6.76052935e-03\n",
      " 3.19597259e-02 5.40870549e-06 8.74993328e-04 8.29765536e-03\n",
      " 8.68747413e-03]\n",
      "\n",
      "5.   99% confidence intervals for the coefficients:\n",
      "- θ̂_0: [-38.5660401058395, -34.653147452375755]\n",
      "- θ̂_1: [0.4266114975800853, 0.4515705861732263]\n",
      "- θ̂_2: [0.008277290830111122, 0.010920002466429816]\n",
      "- θ̂_3: [-0.12072729103575953, -0.0858950544379156]\n",
      "- θ̂_4: [0.5343971958014372, 0.6990631080511346]\n",
      "- θ̂_5: [-2.1566373106025283e-05, 6.300869158370453e-06]\n",
      "- θ̂_6: [-0.006742494117072442, -0.002234271011220248]\n",
      "- θ̂_7: [-0.438729264964255, -0.39597730286204025]\n",
      "- θ̂_8: [-0.4529946736548959, -0.40823425053124895]\n"
     ]
    }
   ],
   "source": [
    "# Freedom degrees for the t-distribution = n - rank(X), where:\n",
    "# - n is the number of observations in the training set\n",
    "# - rank(X) = p + 1 is the number of linearly independent columns in X\n",
    "n = X_train.shape[0]\n",
    "p = X_train.shape[1] - 1  # Number of features excluding the intercept\n",
    "freedom_degrees = n - (p + 1)\n",
    "print(f\"1.   Degrees of freedom for the t-distribution (n-p-1): {freedom_degrees}\\n\")\n",
    "\n",
    "# Estimation of the variance of the error term (σ²)\n",
    "residuals = np.sum((y_train - X_train @ theta_hat)**2)\n",
    "sigma_squared = residuals / freedom_degrees\n",
    "std_dev_error_term = np.sqrt(sigma_squared)\n",
    "print(f\"2.1. Estimated variance of the error term (σ²): {sigma_squared}\\n\")\n",
    "print(f\"2.2. Estimated standard deviation of the error term (σ): {std_dev_error_term}\\n\")\n",
    "\n",
    "# t-value for a 99% confidence interval (1 - alpha = 0.99)\n",
    "alpha = 0.01  # 1 - 0.99 = 0.01\n",
    "t_value = stats.t.ppf(1 - alpha/2, freedom_degrees)\n",
    "print(f\"3.   t-value for a 99% confidence interval: {t_value}\\n\")\n",
    "\n",
    "# Standard error of the coefficients\n",
    "se = std_dev_error_term * np.sqrt(np.diag(np.linalg.inv(X_train.T @ X_train)))\n",
    "print(f\"4.   Standard error of the coefficients (σ_hat √[X.T @ X]^-1_(j,j)):{se}\\n\")\n",
    "\n",
    "# Lower and upper bounds of the 99% confidence interval for each coefficient\n",
    "lower_bounds = theta_hat - t_value * se\n",
    "upper_bounds = theta_hat + t_value * se\n",
    "print(\"5.   99% confidence intervals for the coefficients:\")\n",
    "for i in range(len(theta_hat)):\n",
    "    print(f\"- θ̂_{i}: [{lower_bounds[i]}, {upper_bounds[i]}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3827d53c",
   "metadata": {},
   "source": [
    "### Analysis of Confidence Intervals and Model Performance\n",
    "\n",
    "The confidence intervals computed at the 99% confidence level for the California Housing dataset reveal an important pattern: several coefficients have intervals that contain zero. This observation has significant implications for the regression model:\n",
    "\n",
    "**When a confidence interval contains zero**, it indicates that there is no statistically significant evidence that the corresponding coefficient differs from zero at the 99% confidence level. This implies that the associated feature may not have a meaningful linear relationship with the target variable.\n",
    "\n",
    "**Consequence on Model Performance**: The presence of non-significant coefficients suggests that the dataset contains features with limited predictive power. This explains the moderate R² scores (≈0.58) observed in the California Housing model. With only a subset of truly relevant features among the 8 available, the model's explanatory power is constrained by the inherent noise in real-world housing data.\n",
    "\n",
    "**Key Insight**: This analysis of the California Housing dataset demonstrates the importance of **feature selection and regularization techniques** (Ridge, Lasso, ElasticNet, and PCA) when working with more complex, high-dimensional datasets. These methods effectively address multicollinearity and improve generalization by either eliminating irrelevant features or constraining their coefficients, as will be evidenced in the analysis of the 210-feature dataset in Questions 2-8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085a865",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "**For the rest of the TP, we use the dataset in eCampus `data`. Load and preprocess the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ca2c135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.298173</td>\n",
       "      <td>-0.162249</td>\n",
       "      <td>1.223379</td>\n",
       "      <td>1.355554</td>\n",
       "      <td>1.080171</td>\n",
       "      <td>0.634979</td>\n",
       "      <td>0.298741</td>\n",
       "      <td>0.548270</td>\n",
       "      <td>0.731773</td>\n",
       "      <td>1.018645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588278</td>\n",
       "      <td>0.210106</td>\n",
       "      <td>1.861458</td>\n",
       "      <td>-0.436399</td>\n",
       "      <td>0.279299</td>\n",
       "      <td>-1.416020</td>\n",
       "      <td>-2.332363</td>\n",
       "      <td>0.215096</td>\n",
       "      <td>-0.693319</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166951</td>\n",
       "      <td>-0.338060</td>\n",
       "      <td>-0.618867</td>\n",
       "      <td>0.759366</td>\n",
       "      <td>1.134281</td>\n",
       "      <td>-0.536844</td>\n",
       "      <td>-0.075120</td>\n",
       "      <td>0.970251</td>\n",
       "      <td>-0.327487</td>\n",
       "      <td>0.717310</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251054</td>\n",
       "      <td>-0.825716</td>\n",
       "      <td>0.339139</td>\n",
       "      <td>1.119430</td>\n",
       "      <td>0.225958</td>\n",
       "      <td>-0.822288</td>\n",
       "      <td>0.382838</td>\n",
       "      <td>-0.718829</td>\n",
       "      <td>-0.188993</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.416177</td>\n",
       "      <td>-0.205659</td>\n",
       "      <td>-1.282226</td>\n",
       "      <td>1.675500</td>\n",
       "      <td>1.523746</td>\n",
       "      <td>0.192029</td>\n",
       "      <td>-0.235840</td>\n",
       "      <td>-1.954626</td>\n",
       "      <td>-0.853309</td>\n",
       "      <td>0.892791</td>\n",
       "      <td>...</td>\n",
       "      <td>1.283837</td>\n",
       "      <td>0.372516</td>\n",
       "      <td>-0.652557</td>\n",
       "      <td>-2.579347</td>\n",
       "      <td>0.139267</td>\n",
       "      <td>-1.901196</td>\n",
       "      <td>0.048210</td>\n",
       "      <td>0.220205</td>\n",
       "      <td>0.471588</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.867184</td>\n",
       "      <td>-0.398667</td>\n",
       "      <td>0.093501</td>\n",
       "      <td>0.025971</td>\n",
       "      <td>1.852099</td>\n",
       "      <td>0.789774</td>\n",
       "      <td>0.801775</td>\n",
       "      <td>0.376711</td>\n",
       "      <td>0.853689</td>\n",
       "      <td>0.247953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446582</td>\n",
       "      <td>0.334733</td>\n",
       "      <td>0.399074</td>\n",
       "      <td>-0.884172</td>\n",
       "      <td>0.723819</td>\n",
       "      <td>1.316367</td>\n",
       "      <td>0.088218</td>\n",
       "      <td>0.619496</td>\n",
       "      <td>1.061662</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.193282</td>\n",
       "      <td>-0.936980</td>\n",
       "      <td>-0.725039</td>\n",
       "      <td>0.766078</td>\n",
       "      <td>0.223489</td>\n",
       "      <td>-1.584622</td>\n",
       "      <td>1.146866</td>\n",
       "      <td>0.086136</td>\n",
       "      <td>-0.088780</td>\n",
       "      <td>-0.945066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786157</td>\n",
       "      <td>-1.058179</td>\n",
       "      <td>-0.155788</td>\n",
       "      <td>-0.642504</td>\n",
       "      <td>2.040010</td>\n",
       "      <td>-1.703110</td>\n",
       "      <td>-1.901502</td>\n",
       "      <td>1.778811</td>\n",
       "      <td>-0.489853</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>-0.270323</td>\n",
       "      <td>-0.437638</td>\n",
       "      <td>0.347423</td>\n",
       "      <td>-0.123436</td>\n",
       "      <td>0.344168</td>\n",
       "      <td>-0.777434</td>\n",
       "      <td>-1.380455</td>\n",
       "      <td>0.491346</td>\n",
       "      <td>0.713854</td>\n",
       "      <td>-0.693186</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051364</td>\n",
       "      <td>-0.371945</td>\n",
       "      <td>-0.114830</td>\n",
       "      <td>0.153832</td>\n",
       "      <td>-0.973347</td>\n",
       "      <td>-0.997793</td>\n",
       "      <td>0.158006</td>\n",
       "      <td>-0.139519</td>\n",
       "      <td>1.010518</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>0.872196</td>\n",
       "      <td>0.975497</td>\n",
       "      <td>0.819331</td>\n",
       "      <td>-0.975557</td>\n",
       "      <td>-0.968388</td>\n",
       "      <td>1.029983</td>\n",
       "      <td>-0.079420</td>\n",
       "      <td>-0.130714</td>\n",
       "      <td>0.201144</td>\n",
       "      <td>-2.390860</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.327924</td>\n",
       "      <td>0.350886</td>\n",
       "      <td>-0.305686</td>\n",
       "      <td>-1.292688</td>\n",
       "      <td>0.124676</td>\n",
       "      <td>1.465920</td>\n",
       "      <td>0.663206</td>\n",
       "      <td>1.278693</td>\n",
       "      <td>0.419890</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>-0.032586</td>\n",
       "      <td>-0.571893</td>\n",
       "      <td>0.806842</td>\n",
       "      <td>0.562865</td>\n",
       "      <td>1.194239</td>\n",
       "      <td>-0.345469</td>\n",
       "      <td>0.717316</td>\n",
       "      <td>0.234458</td>\n",
       "      <td>1.546961</td>\n",
       "      <td>0.554013</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.467585</td>\n",
       "      <td>0.584516</td>\n",
       "      <td>-0.281854</td>\n",
       "      <td>-0.618165</td>\n",
       "      <td>0.840381</td>\n",
       "      <td>1.261452</td>\n",
       "      <td>-0.084541</td>\n",
       "      <td>0.301755</td>\n",
       "      <td>0.517624</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-1.529754</td>\n",
       "      <td>0.756967</td>\n",
       "      <td>2.251588</td>\n",
       "      <td>-0.052600</td>\n",
       "      <td>0.502047</td>\n",
       "      <td>0.046229</td>\n",
       "      <td>-1.571494</td>\n",
       "      <td>0.238793</td>\n",
       "      <td>-1.211869</td>\n",
       "      <td>-0.896148</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-2.231379</td>\n",
       "      <td>-0.880398</td>\n",
       "      <td>0.267481</td>\n",
       "      <td>1.036171</td>\n",
       "      <td>-0.962587</td>\n",
       "      <td>0.491072</td>\n",
       "      <td>-1.389069</td>\n",
       "      <td>0.473725</td>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>0.942724</td>\n",
       "      <td>1.389141</td>\n",
       "      <td>-0.028890</td>\n",
       "      <td>-0.803132</td>\n",
       "      <td>0.780272</td>\n",
       "      <td>-2.076596</td>\n",
       "      <td>0.384999</td>\n",
       "      <td>-1.278517</td>\n",
       "      <td>0.544748</td>\n",
       "      <td>-1.933845</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.672230</td>\n",
       "      <td>0.424373</td>\n",
       "      <td>-1.475910</td>\n",
       "      <td>0.198051</td>\n",
       "      <td>-1.129065</td>\n",
       "      <td>1.542559</td>\n",
       "      <td>0.616853</td>\n",
       "      <td>-0.047552</td>\n",
       "      <td>0.897446</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0   -1.298173 -0.162249  1.223379  1.355554  1.080171  0.634979  0.298741   \n",
       "1    0.166951 -0.338060 -0.618867  0.759366  1.134281 -0.536844 -0.075120   \n",
       "2   -0.416177 -0.205659 -1.282226  1.675500  1.523746  0.192029 -0.235840   \n",
       "3    0.867184 -0.398667  0.093501  0.025971  1.852099  0.789774  0.801775   \n",
       "4    1.193282 -0.936980 -0.725039  0.766078  0.223489 -1.584622  1.146866   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "437 -0.270323 -0.437638  0.347423 -0.123436  0.344168 -0.777434 -1.380455   \n",
       "438  0.872196  0.975497  0.819331 -0.975557 -0.968388  1.029983 -0.079420   \n",
       "439 -0.032586 -0.571893  0.806842  0.562865  1.194239 -0.345469  0.717316   \n",
       "440 -1.529754  0.756967  2.251588 -0.052600  0.502047  0.046229 -1.571494   \n",
       "441  0.942724  1.389141 -0.028890 -0.803132  0.780272 -2.076596  0.384999   \n",
       "\n",
       "          7         8         9    ...       201       202       203  \\\n",
       "0    0.548270  0.731773  1.018645  ...  0.588278  0.210106  1.861458   \n",
       "1    0.970251 -0.327487  0.717310  ... -0.251054 -0.825716  0.339139   \n",
       "2   -1.954626 -0.853309  0.892791  ...  1.283837  0.372516 -0.652557   \n",
       "3    0.376711  0.853689  0.247953  ...  0.446582  0.334733  0.399074   \n",
       "4    0.086136 -0.088780 -0.945066  ...  0.786157 -1.058179 -0.155788   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "437  0.491346  0.713854 -0.693186  ... -0.051364 -0.371945 -0.114830   \n",
       "438 -0.130714  0.201144 -2.390860  ... -0.327924  0.350886 -0.305686   \n",
       "439  0.234458  1.546961  0.554013  ... -1.467585  0.584516 -0.281854   \n",
       "440  0.238793 -1.211869 -0.896148  ... -0.000023 -2.231379 -0.880398   \n",
       "441 -1.278517  0.544748 -1.933845  ... -0.672230  0.424373 -1.475910   \n",
       "\n",
       "          204       205       206       207       208       209    210  \n",
       "0   -0.436399  0.279299 -1.416020 -2.332363  0.215096 -0.693319  151.0  \n",
       "1    1.119430  0.225958 -0.822288  0.382838 -0.718829 -0.188993   75.0  \n",
       "2   -2.579347  0.139267 -1.901196  0.048210  0.220205  0.471588  141.0  \n",
       "3   -0.884172  0.723819  1.316367  0.088218  0.619496  1.061662  206.0  \n",
       "4   -0.642504  2.040010 -1.703110 -1.901502  1.778811 -0.489853  135.0  \n",
       "..        ...       ...       ...       ...       ...       ...    ...  \n",
       "437  0.153832 -0.973347 -0.997793  0.158006 -0.139519  1.010518  178.0  \n",
       "438 -1.292688  0.124676  1.465920  0.663206  1.278693  0.419890  104.0  \n",
       "439 -0.618165  0.840381  1.261452 -0.084541  0.301755  0.517624  132.0  \n",
       "440  0.267481  1.036171 -0.962587  0.491072 -1.389069  0.473725  220.0  \n",
       "441  0.198051 -1.129065  1.542559  0.616853 -0.047552  0.897446   57.0  \n",
       "\n",
       "[442 rows x 211 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eb51a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[210]\n",
    "X = df.drop(210, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2192b60d",
   "metadata": {},
   "source": [
    "### Question 2.a\n",
    "**Separate the data in train and test sets: save one fourth of the data as testing (you can use `train_test_split` from `sklearn.model_selection`) and standardize both the training and testing sets using the `fit_transform` and `transform` functions in `sklearn.preprocessing.StandardScaler`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3cc13bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the scaled datasets:\n",
      "X_train_scaled_shape: (331, 210)\n",
      "X_test_scaled_shape: (111, 210)\n",
      "y_train_scaled_shape: (331,)\n",
      "y_test_scaled_shape: (111,)\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scaling the training data with fit_transform and the test data \n",
    "# with the parameters learned from the training data using transform \n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "print(\"Shapes of the scaled datasets:\")\n",
    "print(f\"X_train_scaled_shape: {X_train_scaled.shape}\")\n",
    "print(f\"X_test_scaled_shape: {X_test_scaled.shape}\")\n",
    "print(f\"y_train_scaled_shape: {y_train_scaled.shape}\")\n",
    "print(f\"y_test_scaled_shape: {y_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb9d126",
   "metadata": {},
   "source": [
    "### Question 2.b\n",
    "**Fit a regular OLS.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29103d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular OLS metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Test Set</th>\n",
       "      <th>Train Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSE</td>\n",
       "      <td>0.543767</td>\n",
       "      <td>0.38102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R2 Score</td>\n",
       "      <td>0.315038</td>\n",
       "      <td>0.61898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Metric  Test Set  Train Set\n",
       "0       MSE  0.543767    0.38102\n",
       "1  R2 Score  0.315038    0.61898"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predictions on the test set for performance evaluation\n",
    "y_pred_test_scaled = lin_reg.predict(X_test_scaled)\n",
    "\n",
    "# Predictions on the training set for performance evaluation\n",
    "y_pred_train_scaled = lin_reg.predict(X_train_scaled)\n",
    "\n",
    "# Evaluation of the performance of the regular OLS regression\n",
    "# model on both the test and train sets using MSE and R2 Score\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['MSE', 'R2 Score'],\n",
    "    'Test Set': [MSE(y_test_scaled, y_pred_test_scaled), R2(y_test_scaled, y_pred_test_scaled)],\n",
    "    'Train Set': [MSE(y_train_scaled, y_pred_train_scaled), R2(y_train_scaled, y_pred_train_scaled)]\n",
    "})\n",
    "print(\"Regular OLS metrics:\")\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3e1af",
   "metadata": {},
   "source": [
    "### Comments about the OLS metrics\n",
    "The OLS model achieves moderate predictive performance on the test set (R² ≈ 0.32) but performs substantially better on the training set (R² ≈ 0.62), indicating a notable generalization gap. This suggests either mild overfitting, dataset shift, or that a purely linear relationship is insufficient for the underlying signal. Further validation (e.g., cross-validation) and regularization or feature engineering may improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f44a823",
   "metadata": {},
   "source": [
    "---\n",
    "# Variable selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ac791a",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "**Program the method of forward variable selection based on hypothesis tests for regression coefﬁcients. This method starts from an empty set of variables $S$ and at each iteration selects one variable relevant for predicting $y$ and includes it in the set $S$. It runs until a halting condition is met. The coding process is as follows:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f2bdc",
   "metadata": {},
   "source": [
    "### Question 3.a\n",
    "\n",
    "**Develop a function that, given a dataset $X \\in \\mathbb{R}^{n \\times p}$ and $y$, ﬁts $p$ linear regression models, each using only feature $X_{j}$ to predict $y$. For each model, conduct a test of no effect, as discussed in session 3, and compute the p-value of the test. This function should return the coeﬃcient with the smallest p-value. Explain the signiﬁcance of the p-value in this context.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bb366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_p_value_feature(X, y):\n",
    "\n",
    "    list_of_p_values = []\n",
    "    min_p_value_info = {\"Feature Index\": -1, \"P Value\": 10}\n",
    "\n",
    "    for feature_index in range(X.shape[1]):\n",
    "\n",
    "        degrees_freedom = y.shape[0] - 2\n",
    "        linear_model = LinearRegression(fit_intercept=False)\n",
    "        linear_model.fit(X[:, feature_index].reshape(-1, 1), y)\n",
    "\n",
    "        residuals = y - \\\n",
    "            linear_model.predict(X[:, feature_index].reshape(-1, 1))\n",
    "        residual_variance = residuals.T @ residuals / degrees_freedom\n",
    "\n",
    "        t_stat = linear_model.coef_[\n",
    "            0]/np.sqrt(residual_variance*(1/(X[:, feature_index].T @ X[:, feature_index])))\n",
    "\n",
    "        p_val = 2 * (1 - stats.t.cdf(np.abs(t_stat), degrees_freedom))\n",
    "\n",
    "        list_of_p_values.append(p_val)\n",
    "\n",
    "        if p_val < min_p_value_info[\"P Value\"]:\n",
    "            min_p_value_info[\"Feature Index\"] = feature_index\n",
    "            min_p_value_info[\"P Value\"] = p_val\n",
    "\n",
    "    linear_model.fit(X[:, min_p_value_info[\"Feature Index\"]].reshape(-1, 1), y)\n",
    "    feature_prediction = linear_model.predict(\n",
    "        X[:, min_p_value_info[\"Feature Index\"]].reshape(-1, 1))\n",
    "\n",
    "    return feature_prediction, min_p_value_info, list_of_p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc60f2",
   "metadata": {},
   "source": [
    "### Question 3.b\n",
    "\n",
    "**Apply the function iteratively. At each iteration, select the feature $X_{f}$ with the smallest p-value and:**\n",
    "* **Include it in the set $S$.**\n",
    "* **Remove it from $X$.**\n",
    "* **Subtract from $y$ the residuals of the model ﬁt with feature $X_{f}$. Elaborate on the reason for subtracting the predictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50715ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_current = X_train_scaled\n",
    "y_current = y_train_scaled\n",
    "p_values_per_iteration = []\n",
    "p_values_list = []\n",
    "selected_indices = []\n",
    "selected_features = []\n",
    "iteration_count = 0\n",
    "\n",
    "while True:\n",
    "\n",
    "    model_prediction, min_p_value_result, iteration_p_values = find_min_p_value_feature(\n",
    "        X_current, y_current)\n",
    "\n",
    "    p_values_list.append(min_p_value_result[\"P Value\"])\n",
    "    p_values_per_iteration.append(iteration_p_values)\n",
    "\n",
    "    if min_p_value_result[\"P Value\"] > 0.05:\n",
    "        break\n",
    "\n",
    "    selected_features.append(X_current[:, min_p_value_result[\"Feature Index\"]])\n",
    "    selected_indices.append(min_p_value_result[\"Feature Index\"])\n",
    "\n",
    "    X_current = np.delete(\n",
    "        X_current, min_p_value_result[\"Feature Index\"], axis=1)\n",
    "\n",
    "    y_current = y_current - model_prediction\n",
    "\n",
    "    iteration_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1170de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(p_values_list, 'o-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('P-value')\n",
    "plt.title('P-values for each iteration')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, iteration_p_values in enumerate(p_values_per_iteration[:5], start=1):\n",
    "    plt.plot(iteration_p_values, 'o', label=f'Iteration {i}')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('P-value')\n",
    "plt.title('P-values for each coefficient for the first 5 iterations')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6b3a8a",
   "metadata": {},
   "source": [
    "### Question 3.c\n",
    "**Add a halting condition to the algorithm: Stop adding features to the set $S$ when the p-value exceeds $0.05$. Plot the p-values for every coeﬃcient for the ﬁrst 5 iterations (all in the same plot).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad848be",
   "metadata": {},
   "source": [
    "From the plot above, it makes no sense to plot the p-values for the first 5 iterations, since the p-values are all below 0.05. Therefore, the plot of the p-values was done for more iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e41c0b6",
   "metadata": {},
   "source": [
    "---\n",
    "# Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd88ea14",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "**Run Ridge Regression using `scikit-learn` on the training set. Run the code for $30$ different values of the penalty parameter, which should be on a logarithmic scale between $10^{-1}$ and $10^{6}$. Display two subplots at the end:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79bc082",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-1, 6, 30)\n",
    "\n",
    "coefficients = []\n",
    "r2_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "\n",
    "    ridge.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "    coefficients.append(ridge.coef_.flatten())\n",
    "    r2_scores.append(r2_score(y_test_scaled, ridge.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c971183b",
   "metadata": {},
   "source": [
    "### Question 4.a and 4.b\n",
    "\n",
    "**The first subplot should show the evolution of the coefficients for each different value of the penalty parameter.**\n",
    "\n",
    "**The second subplot should display the evolution of the R-squared coefficient at each of the $30$ iterations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e8882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two subplots\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# Plot alpha values against coefficients\n",
    "ax[0].plot(alphas, coefficients)\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_xlabel('Alpha')\n",
    "ax[0].set_ylabel('Coefficients')\n",
    "\n",
    "# Plot alpha values against R-squared scores\n",
    "ax[1].plot(alphas, r2_scores)\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].set_xlabel('Alpha')\n",
    "ax[1].set_ylabel('R-squared')\n",
    "ax[1].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c012fe22",
   "metadata": {},
   "source": [
    "**Since we are going to perform similar tasks for Lasso and Elastic Net, it is mandatory to write this code as an independent function that can be parameterized for each speciﬁc case.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d2f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, alpha_start, alpha_end, X_train, y_train, X_test, y_test):\n",
    "\n",
    "    alphas = np.logspace(alpha_start, alpha_end, 30)\n",
    "\n",
    "    coefficients = []\n",
    "    r2_scores = []\n",
    "\n",
    "    for alpha in alphas:\n",
    "\n",
    "        reg = model(alpha=alpha)\n",
    "        reg.fit(X_train, y_train)\n",
    "        coefficients.append(reg.coef_)\n",
    "        r2_scores.append(r2_score(y_test, reg.predict(X_test)))\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "    ax[0].plot(alphas, coefficients)\n",
    "    ax[0].set_xscale('log')\n",
    "    ax[0].set_xlabel('Alpha')\n",
    "    ax[0].set_ylabel('Coefficients')\n",
    "\n",
    "    ax[1].plot(alphas, r2_scores)\n",
    "    ax[1].set_xscale('log')\n",
    "    ax[1].set_xlabel('Alpha')\n",
    "    ax[1].set_ylabel('R-squared')\n",
    "    ax[1].grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6549fe9",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "**Run the code for Lasso as explained in Point $4$. Run the code for $30$ different values of the penalty parameter, which should be on a logarithmic scale between $10^{-3}$ and $10^{2}$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb57e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Lasso regression\n",
    "run_model(Lasso, -3, 2, X_train_scaled,\n",
    "          y_train_scaled, X_test_scaled, y_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72c6233",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "**Run the code for ElasticNet as explained in Point $4$. Run the code for $30$ different values of the penalty parameter, which should be on a logarithmic scale between $10^{-3}$ and $10^{2}$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a0df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code for ElasticNet\n",
    "run_model(ElasticNet, -3, 2, X_train_scaled,\n",
    "          y_train_scaled, X_test_scaled, y_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd74d96c",
   "metadata": {},
   "source": [
    "---\n",
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f604e9e",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "**Compute the singular value decomposition of the covariance matrix. For consistency in the notation use $U, s, V = SVD(X^{T}X)$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed4faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance matrix\n",
    "cov_matrix = np.dot(X_train_scaled.T, X_train_scaled)\n",
    "\n",
    "# SVD of the covariance matrix\n",
    "U, s, V = np.linalg.svd(cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704d1ee1",
   "metadata": {},
   "source": [
    "### Question 7.a\n",
    "\n",
    "**Plot a heatmap of the covariance matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(cov_matrix, annot=False, fmt=\".1f\",\n",
    "            cmap='viridis', square=True, linewidths=.5)\n",
    "plt.title('Heatmap of Covariance Matrix')\n",
    "\n",
    "# Assuming we want to show every 10th label for a matrix of size 200\n",
    "ticks = np.arange(0, len(cov_matrix), 10)\n",
    "plt.xticks(ticks, labels=ticks)\n",
    "plt.yticks(ticks, labels=ticks)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ffb092",
   "metadata": {},
   "source": [
    "### Question 7.b\n",
    "\n",
    "**Compute the PCA for the data using the SVD.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cd6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_PCA = PCA(n_components=len(s)).fit_transform(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50070be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_PCA(X, k):\n",
    "    X_PCA = X @ U[:, :k]\n",
    "    return s[:k], X_PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e7a3fe",
   "metadata": {},
   "source": [
    "### Question 7.c\n",
    "\n",
    "**Plot the amount of variance explained by the first $k$ components for $k \\in 2..p$. How many variables do we need to explain more than $90\\%$ of the variance?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45019d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the variance explained by each principal component\n",
    "variance_explained = (s ** 2) / np.sum(s ** 2)\n",
    "\n",
    "# Plot the variance explained by the first k components\n",
    "plt.figure(figsize=(10, 6), dpi=100)\n",
    "plt.plot(range(2, len(variance_explained) + 1),\n",
    "         np.cumsum(variance_explained[1:]), linewidth=2)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Variance Explained')\n",
    "plt.title('Cumulative Variance Explained by Principal Components')\n",
    "plt.grid()\n",
    "\n",
    "# Find the number of components needed to explain more than 90% of the variance\n",
    "num_components = np.argmax(np.cumsum(variance_explained) > 0.9) + 2\n",
    "plt.axvline(x=num_components, color='r', linestyle='--')\n",
    "plt.text(num_components + 1, 0.85, f'{num_components} components', color='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e3c6b7",
   "metadata": {},
   "source": [
    "In the displayed graph, it's clear that slightly less than $50$ components, are required to explain most of the variance in the data, as the curve shows a plateau beyond this point. This method is known as the *Elbow Method*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0597c5e8",
   "metadata": {},
   "source": [
    "### Question 7.d\n",
    "\n",
    "**Plot the projected data with $k = 2$ using as color the value of $y$ and interpret the plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc2673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "principal_components = X_train_scaled @ U[:, :k]\n",
    "\n",
    "plt.figure(figsize=(10, 6), dpi=100)\n",
    "plt.scatter(principal_components[:, 0],\n",
    "            principal_components[:, 1], c=y_train_scaled)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Projected Data with k=2')\n",
    "plt.colorbar(label='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552550d3",
   "metadata": {},
   "source": [
    "### Question 7.e\n",
    "\n",
    "**Plot the the two first principal directions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1f13f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], alpha=0.4)\n",
    "\n",
    "# First principal direction\n",
    "plt.quiver(0, 0, V[:, 0] @ V[:, 0], V[:, 0] @ V[:, 1],\n",
    "           angles='xy', scale_units='xy', scale=1, color='r', label='PC1')\n",
    "\n",
    "# Second principal direction\n",
    "plt.quiver(0, 0, V[:, 1] @ V[:, 0], V[:, 1] @ V[:, 1],\n",
    "           angles='xy', scale_units='xy', scale=1, color='b', label='PC2')\n",
    "\n",
    "plt.title('The Two Principal Directions')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fae6b3",
   "metadata": {},
   "source": [
    "### Question 7.f\n",
    "\n",
    "**Run OLS on the projected data (PCR) using $k$ components for $k < 50$. Select the $k$ that returns the best score of the OLS model and plot the evolution of the scores with $k$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa25fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "best_k = 0\n",
    "scores = []\n",
    "\n",
    "for k in range(1, 50):\n",
    "    # Project the data onto k principal components\n",
    "    principal_components = X_train_scaled @ U[:, :k]\n",
    "\n",
    "    # Fit OLS model\n",
    "    ols = LinearRegression()\n",
    "    ols.fit(principal_components, y_train_scaled)\n",
    "\n",
    "    # Predict using the OLS model\n",
    "    y_pred_scaled = ols.predict(principal_components)\n",
    "\n",
    "    # Calculate R-squared score\n",
    "    score = r2_score(y_train_scaled, y_pred_scaled)\n",
    "\n",
    "    # Store the score and update the best score and k if necessary\n",
    "    scores.append(score)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_k = k\n",
    "\n",
    "# Plot the evolution of the scores with k\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(range(1, 50), scores, marker='o')\n",
    "plt.xlabel('Number of Components (k)')\n",
    "plt.ylabel('R2 Score')\n",
    "plt.title('Evolution of R-squared Score with Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print the best k and its corresponding score\n",
    "print(f\"The best k is {best_k} with an R-squared score of {best_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b662a",
   "metadata": {},
   "source": [
    "---\n",
    "# Comparison of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84d3e15",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "**Summarize the results of the models and elaborate in their main characteristics. Plot all the training and testing errors for all the models considered and elaborate on the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7832356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Initialize the models\n",
    "lr = LinearRegression()\n",
    "ridge = Ridge()\n",
    "lasso = Lasso()\n",
    "elastic_net = ElasticNet()\n",
    "# Adjust n_components as needed\n",
    "pca = make_pipeline(PCA(n_components=10), LinearRegression())\n",
    "fvs = make_pipeline(SelectKBest(score_func=f_regression, k=10),\n",
    "                    LinearRegression())  # Adjust k as needed\n",
    "\n",
    "# Fit and predict for each model\n",
    "models = [lr, ridge, lasso, elastic_net, pca, fvs]\n",
    "y_preds_train = []\n",
    "y_preds_test = []\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds_train.append(model.predict(X_train))\n",
    "    y_preds_test.append(model.predict(X_test))\n",
    "\n",
    "# Calculate the metrics\n",
    "scores_test = [r2_score(y_test, y_pred) for y_pred in y_preds_test]\n",
    "mses_test = [mean_squared_error(y_test, y_pred) for y_pred in y_preds_test]\n",
    "\n",
    "scores_train = [r2_score(y_train, y_pred) for y_pred in y_preds_train]\n",
    "mses_train = [mean_squared_error(y_train, y_pred) for y_pred in y_preds_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f0400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the model names\n",
    "models = ['Linear Regression', 'Ridge', 'Lasso',\n",
    "          'ElasticNet', 'PCA', 'Forward Variable Selection']\n",
    "\n",
    "# Plot R² scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models, scores_train, color='blue', width=0.4, label='Training')\n",
    "plt.bar(np.arange(len(models)) + 0.4, scores_test,\n",
    "        color='green', width=0.4, label='Testing')\n",
    "plt.title('R² - Training vs Testing')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('R²')\n",
    "plt.xticks(np.arange(len(models)) + 0.2, models)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot MSEs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models, mses_train, color='blue', width=0.4, label='Training')\n",
    "plt.bar(np.arange(len(models)) + 0.4, mses_test,\n",
    "        color='green', width=0.4, label='Testing')\n",
    "plt.title('MSE - Training vs Testing')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('MSE')\n",
    "plt.xticks(np.arange(len(models)) + 0.2, models)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
